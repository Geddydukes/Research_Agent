# Agentic Research Knowledge Graph

A TypeScript backend for a 5-agent pipeline that processes academic papers into a semantic knowledge graph.

## Architecture

The system consists of 5 sequential agents with strict boundaries:

1. **Ingestion Agent**: Parses papers into structured sections (verbatim extraction, no semantics)
2. **Entity Extraction Agent**: Extracts max 10 entities per paper (max 4 per section, max 2 metrics)
3. **Relationship Extraction Agent**: Proposes edges with strict evidence and named baselines
4. **Validation Agent**: Enforces graph invariants with confidence tracking
5. **Graph Reasoning Agent**: Infers non-obvious insights with deduplication and corpus dampening

## Features

- **Agent isolation**: Agents cannot see each other's internals
- **Failure isolation**: One paper fails, pipeline continues
- **Schema validation**: All outputs validated against Zod schemas
- **Retry with feedback**: Max 2 retries with schema error injection
- **Timeouts**: 30s per agent call
- **Precision over recall**: Better to miss than hallucinate
- **Auditability**: Full provenance and confidence tracking

## Prerequisites

- Node.js 20+
- TypeScript 5.0+
- Supabase account and project
- Anthropic API key

## Setup

### 1. Install Dependencies

```bash
npm install
```

### 2. Configure Environment Variables

Create a `.env` file in the project root:

```bash
ANTHROPIC_API_KEY=your_anthropic_api_key_here
SUPABASE_URL=your_supabase_url_here
SUPABASE_SERVICE_ROLE_KEY=your_supabase_service_role_key_here
```

### 3. Set Up Database

1. Create a new Supabase project or use an existing one
2. Run the SQL schema from `src/db/schema.sql` in your Supabase SQL editor
3. Run migrations to seed node types:

```bash
npm run migrate
```

### 4. Build the Project

```bash
npm run build
```

## Usage

### Process a Paper

```typescript
import { runPipeline, createDatabaseClient } from './dist/index';

const db = createDatabaseClient();
const result = await runPipeline({
  paper_id: '2308.04079',
  title: '3D Gaussian Splatting for Real-Time Radiance Field Rendering',
  raw_text: 'Abstract\n\nWe introduce 3D Gaussian Splatting...',
  metadata: {
    year: 2023,
    authors: ['Kerbl', 'Kopanas', 'Leimkühler'],
  },
}, db);

if (result.success) {
  console.log('Stats:', result.stats);
}
```

### Run Example

```bash
npm run dev
```

## Testing

### Run Unit Tests

```bash
npm test
```

### Run Integration Tests

```bash
# Make sure environment variables are set
ts-node tests/agent.test.ts
ts-node tests/pipeline.test.ts
```

## Project Structure

```
Research_agent/
├── src/
│   ├── agents/
│   │   ├── config.ts          # Agent configuration
│   │   ├── schemas.ts          # Zod schemas for all agents
│   │   ├── runAgent.ts         # Core agent execution with retry logic
│   │   ├── errors.ts           # Custom error types
│   │   └── prompts/           # Agent prompts (replace with revised prompts)
│   ├── db/
│   │   ├── schema.sql          # Database schema
│   │   ├── client.ts           # Supabase client with typed queries
│   │   └── migrations.ts       # Database migrations
│   ├── pipeline/
│   │   ├── runPipeline.ts      # Main pipeline orchestration
│   │   └── types.ts            # Pipeline types
│   └── index.ts                # Main entry point
├── tests/
│   ├── fixtures/
│   │   └── sample_paper.json   # Sample paper for testing
│   ├── agent.test.ts           # Agent unit tests
│   └── pipeline.test.ts        # End-to-end pipeline tests
├── package.json
├── tsconfig.json
└── README.md
```

## Database Schema

The system uses the following main tables:

- `papers`: Paper metadata
- `paper_sections`: Extracted sections from papers
- `nodes`: Entities (methods, datasets, metrics, etc.)
- `edges`: Relationships between entities
- `entity_mentions`: Tracks which papers mention which entities
- `inferred_insights`: Insights generated by the reasoning agent
- `node_type_registry`: Registry of valid entity types

## Agent Configuration

Default configuration in `src/agents/config.ts`:

- `maxRetries`: 2
- `timeoutMs`: 30000 (30 seconds)
- `anthropicModel`: 'claude-sonnet-4-20250514'
- `maxTokens`: 4000

Confidence thresholds:
- `reject`: < 0.3
- `review`: 0.3 - 0.6
- `accept`: > 0.6

## Customizing Prompts

Replace the placeholder prompts in `src/agents/prompts/` with your revised prompts. Each file exports a const string that is used as the system prompt for that agent.

## Error Handling

The pipeline uses structured error handling:

- `TimeoutError`: Agent call exceeded timeout
- `SchemaValidationError`: Schema validation failed after retries
- `AgentExecutionError`: Other agent execution errors

All errors are logged with context and the pipeline continues processing other papers.

## Logging

The system uses structured logging with prefixes:

```
[Ingestion] Processing paper_2308.04079 (attempt 1/3)
[Ingestion] Success: 5 sections extracted in 2.3s
[EntityExtraction] Extracted 8 entities with avg confidence 0.82
[Validation] Approved: 7/8 entities, 12/15 edges (3 flagged for review)
[Reasoning] Generated 4 insights (2 transitive, 1 cluster, 1 anomaly)
```

## Development

### Type Safety

The project uses strict TypeScript with:
- `noImplicitAny`: true
- `strictNullChecks`: true
- All types inferred from Zod schemas

### Code Style

- Functional over OOP
- Single responsibility per file
- Exported consts for prompts/schemas
- Comments explain why, not what

## License

MIT


